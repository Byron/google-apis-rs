initSidebarItems({"enum":[["Scope","Identifies the an OAuth2 authorization scope. A scope is needed when requesting an authorization token."]],"struct":[["ListOperationsResponse","The response message for Operations.ListOperations."],["LongRunningRecognizeRequest","The top-level message sent by the client for the `LongRunningRecognize` method."],["Operation","This resource represents a long-running operation that is the result of a network API call."],["OperationGetCall","Gets the latest state of a long-running operation. Clients can use this method to poll the operation result at intervals as recommended by the API service."],["OperationListCall","Lists operations that match the specified filter in the request. If the server doesn't support this method, it returns `UNIMPLEMENTED`. NOTE: the `name` binding allows API services to override the binding to use different resource name schemes, such as `users/*/operations`. To override the binding, API services can add a binding such as `\"/v1/{name=users/*}/operations\"` to their service configuration. For backwards compatibility, the default name includes the operations collection id, however overriding users must ensure the name binding is the parent resource, without the operations collection id."],["OperationMethods","A builder providing access to all methods supported on operation resources. It is not used directly, but through the `Speech` hub."],["RecognitionAudio","Contains audio data in the encoding specified in the `RecognitionConfig`. Either `content` or `uri` must be supplied. Supplying both or neither returns google.rpc.Code.INVALID_ARGUMENT. See content limits."],["RecognitionConfig","Provides information to the recognizer that specifies how to process the request."],["RecognitionMetadata","Description of audio data to be recognized."],["RecognizeRequest","The top-level message sent by the client for the `Recognize` method."],["RecognizeResponse","The only message returned to the client by the `Recognize` method. It contains the result as zero or more sequential `SpeechRecognitionResult` messages."],["SpeakerDiarizationConfig","Config to enable speaker diarization."],["Speech","Central instance to access all Speech related resource activities"],["SpeechContext","Provides \"hints\" to the speech recognizer to favor specific words and phrases in the results."],["SpeechLongrunningrecognizeCall","Performs asynchronous speech recognition: receive results via the google.longrunning.Operations interface. Returns either an `Operation.error` or an `Operation.response` which contains a `LongRunningRecognizeResponse` message. For more information on asynchronous speech recognition, see the how-to."],["SpeechMethods","A builder providing access to all methods supported on speech resources. It is not used directly, but through the `Speech` hub."],["SpeechRecognitionAlternative","Alternative hypotheses (a.k.a. n-best list)."],["SpeechRecognitionResult","A speech recognition result corresponding to a portion of the audio."],["SpeechRecognizeCall","Performs synchronous speech recognition: receive results after all audio has been sent and processed."],["Status","The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by gRPC. Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the API Design Guide."],["WordInfo","Word-specific information for recognized words."]]});