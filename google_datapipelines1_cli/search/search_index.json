{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The datapipelines1 command-line interface (CLI) allows to use most features of the Google Datapipelines service from the comfort of your terminal. By default all output is printed to standard out, but flags can be set to direct it into a file independent of your shell's capabilities. Errors will be printed to standard error, and cause the program's exit code to be non-zero. If data-structures are requested, these will be returned as pretty-printed JSON, to be useful as input to other tools. Everything else about the Datapipelines API can be found at the official documentation site . Installation and Source Code Install the command-line interface with cargo using: cargo install google-datapipelines1-cli Find the source code on github . Usage This documentation was generated from the Datapipelines API at revision 20240225 . The CLI is at version 5.0.4 . datapipelines1 [options] projects locations-pipelines-create <parent> (-r <kv>)... [-p <v>]... [-o <out>] locations-pipelines-delete <name> [-p <v>]... [-o <out>] locations-pipelines-get <name> [-p <v>]... [-o <out>] locations-pipelines-jobs-list <parent> [-p <v>]... [-o <out>] locations-pipelines-list <parent> [-p <v>]... [-o <out>] locations-pipelines-patch <name> (-r <kv>)... [-p <v>]... [-o <out>] locations-pipelines-run <name> (-r <kv>)... [-p <v>]... [-o <out>] locations-pipelines-stop <name> (-r <kv>)... [-p <v>]... [-o <out>] datapipelines1 --help Configuration: [--scope <url>]... Specify the authentication a method should be executed in. Each scope requires the user to grant this application permission to use it. If unset, it defaults to the shortest scope url for a particular method. --config-dir <folder> A directory into which we will store our persistent data. Defaults to a user-writable directory that we will create during the first invocation. [default: ~/.google-service-cli] Configuration The program will store all persistent data in the ~/.google-service-cli directory in JSON files prefixed with datapipelines1- . You can change the directory used to store configuration with the --config-dir flag on a per-invocation basis. More information about the various kinds of persistent data are given in the following paragraphs. Authentication Most APIs require a user to authenticate any request. If this is the case, the scope determines the set of permissions granted. The granularity of these is usually no more than read-only or full-access . If not set, the system will automatically select the smallest feasible scope, e.g. when invoking a method that is read-only, it will ask only for a read-only scope. You may use the --scope flag to specify a scope directly. All applicable scopes are documented in the respective method's CLI documentation. The first time a scope is used, the user is asked for permission. Follow the instructions given by the CLI to grant permissions, or to decline. If a scope was authenticated by the user, the respective information will be stored as JSON in the configuration directory, e.g. ~/.google-service-cli/datapipelines1-token-<scope-hash>.json . No manual management of these tokens is necessary. To revoke granted authentication, please refer to the official documentation . Application Secrets In order to allow any application to use Google services, it will need to be registered using the Google Developer Console . APIs the application may use are then enabled for it one by one. Most APIs can be used for free and have a daily quota. To allow more comfortable usage of the CLI without forcing anyone to register an own application, the CLI comes with a default application secret that is configured accordingly. This also means that heavy usage all around the world may deplete the daily quota. You can workaround this limitation by putting your own secrets file at this location: ~/.google-service-cli/datapipelines1-secret.json , assuming that the required datapipelines API was enabled for it. Such a secret file can be downloaded in the Google Developer Console at APIs & auth -> Credentials -> Download JSON and used as is. Learn more about how to setup Google projects and enable APIs using the official documentation . Debugging Even though the CLI does its best to provide usable error messages, sometimes it might be desirable to know what exactly led to a particular issue. This is done by allowing all client-server communication to be output to standard error as-is . The --debug flag will print errors using the Debug representation to standard error. You may consider redirecting standard error into a file for ease of use, e.g. datapipelines1 --debug <resource> <method> [options] 2>debug.txt .","title":"Home"},{"location":"#installation-and-source-code","text":"Install the command-line interface with cargo using: cargo install google-datapipelines1-cli Find the source code on github .","title":"Installation and Source Code"},{"location":"#usage","text":"This documentation was generated from the Datapipelines API at revision 20240225 . The CLI is at version 5.0.4 . datapipelines1 [options] projects locations-pipelines-create <parent> (-r <kv>)... [-p <v>]... [-o <out>] locations-pipelines-delete <name> [-p <v>]... [-o <out>] locations-pipelines-get <name> [-p <v>]... [-o <out>] locations-pipelines-jobs-list <parent> [-p <v>]... [-o <out>] locations-pipelines-list <parent> [-p <v>]... [-o <out>] locations-pipelines-patch <name> (-r <kv>)... [-p <v>]... [-o <out>] locations-pipelines-run <name> (-r <kv>)... [-p <v>]... [-o <out>] locations-pipelines-stop <name> (-r <kv>)... [-p <v>]... [-o <out>] datapipelines1 --help Configuration: [--scope <url>]... Specify the authentication a method should be executed in. Each scope requires the user to grant this application permission to use it. If unset, it defaults to the shortest scope url for a particular method. --config-dir <folder> A directory into which we will store our persistent data. Defaults to a user-writable directory that we will create during the first invocation. [default: ~/.google-service-cli]","title":"Usage"},{"location":"#configuration","text":"The program will store all persistent data in the ~/.google-service-cli directory in JSON files prefixed with datapipelines1- . You can change the directory used to store configuration with the --config-dir flag on a per-invocation basis. More information about the various kinds of persistent data are given in the following paragraphs.","title":"Configuration"},{"location":"#authentication","text":"Most APIs require a user to authenticate any request. If this is the case, the scope determines the set of permissions granted. The granularity of these is usually no more than read-only or full-access . If not set, the system will automatically select the smallest feasible scope, e.g. when invoking a method that is read-only, it will ask only for a read-only scope. You may use the --scope flag to specify a scope directly. All applicable scopes are documented in the respective method's CLI documentation. The first time a scope is used, the user is asked for permission. Follow the instructions given by the CLI to grant permissions, or to decline. If a scope was authenticated by the user, the respective information will be stored as JSON in the configuration directory, e.g. ~/.google-service-cli/datapipelines1-token-<scope-hash>.json . No manual management of these tokens is necessary. To revoke granted authentication, please refer to the official documentation .","title":"Authentication"},{"location":"#application-secrets","text":"In order to allow any application to use Google services, it will need to be registered using the Google Developer Console . APIs the application may use are then enabled for it one by one. Most APIs can be used for free and have a daily quota. To allow more comfortable usage of the CLI without forcing anyone to register an own application, the CLI comes with a default application secret that is configured accordingly. This also means that heavy usage all around the world may deplete the daily quota. You can workaround this limitation by putting your own secrets file at this location: ~/.google-service-cli/datapipelines1-secret.json , assuming that the required datapipelines API was enabled for it. Such a secret file can be downloaded in the Google Developer Console at APIs & auth -> Credentials -> Download JSON and used as is. Learn more about how to setup Google projects and enable APIs using the official documentation .","title":"Application Secrets"},{"location":"#debugging","text":"Even though the CLI does its best to provide usable error messages, sometimes it might be desirable to know what exactly led to a particular issue. This is done by allowing all client-server communication to be output to standard error as-is . The --debug flag will print errors using the Debug representation to standard error. You may consider redirecting standard error into a file for ease of use, e.g. datapipelines1 --debug <resource> <method> [options] 2>debug.txt .","title":"Debugging"},{"location":"projects_locations-pipelines-create/","text":"Creates a pipeline. For a batch pipeline, you can pass scheduler information. Data Pipelines uses the scheduler information to create an internal scheduler that runs jobs periodically. If the internal scheduler is not configured, you can use RunPipeline to run jobs. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-create ... Required Scalar Argument <parent> (string) Required. The location name. For example: projects/PROJECT_ID/locations/LOCATION_ID . Required Request Value The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1Pipeline: create-time: string display-name: string job-count: integer last-update-time: string name: string pipeline-sources: { string: string } schedule-info: next-job-time: string schedule: string time-zone: string scheduler-service-account-email: string state: string type: string workload: dataflow-flex-template-request: launch-parameter: container-spec-gcs-path: string environment: additional-experiments: [string] additional-user-labels: { string: string } enable-streaming-engine: boolean flexrs-goal: string ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string launch-options: { string: string } parameters: { string: string } transform-name-mappings: { string: string } update: boolean location: string project-id: string validate-only: boolean dataflow-launch-template-request: gcs-path: string launch-parameters: environment: additional-experiments: [string] additional-user-labels: { string: string } bypass-temp-dir-validation: boolean enable-streaming-engine: boolean ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string parameters: { string: string } transform-name-mapping: { string: string } update: boolean location: string project-id: string validate-only: boolean can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time. -r . create-time=et Output only. Immutable. The timestamp when the pipeline was initially created. Set by the Data Pipelines service. display-name=magna Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_). job-count=90 Output only. Number of jobs. last-update-time=ipsum Output only. Immutable. The timestamp when the pipeline was last modified. Set by the Data Pipelines service. name=voluptua. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . * PROJECT_ID can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see Identifying projects . * LOCATION_ID is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling google.cloud.location.Locations.ListLocations . Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in App Engine regions . * PIPELINE_ID is the ID of the pipeline. Must be unique for the selected project and location. pipeline-sources=key=at Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation. the value will be associated with the given key schedule-info next-job-time=sanctus Output only. When the next Scheduler job is going to run. schedule=sed Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler. time-zone=amet. Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed. .. scheduler-service-account-email=takimata Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used. state=amet. Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests. type=duo Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline. workload.dataflow-flex-template-request.launch-parameter container-spec-gcs-path=ipsum Cloud Storage path to a file with a JSON-serialized ContainerSpec as content. environment additional-experiments=gubergren Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=lorem Additional user labels to be specified for the job. Keys and values must follow the restrictions specified in the labeling restrictions . An object containing a list of key/value pairs. Example: { &#34;name&#34;: &#34;wrench&#34;, &#34;mass&#34;: &#34;1kg&#34;, &#34;count&#34;: &#34;3&#34; } . the value will be associated with the given key enable-streaming-engine=false Whether to enable Streaming Engine for the job. flexrs-goal=dolor Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs ip-configuration=ea Configuration for VM IPs. kms-key-name=ipsum Name for the Cloud KMS key for the job. Key format is: projects//locations//keyRings//cryptoKeys/ machine-type=invidunt The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=54 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=duo Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=51 The initial number of Compute Engine instances for the job. service-account-email=sed The email address of the service account to run the job as. subnetwork=ut Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=gubergren The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=rebum. The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, defaults to the control plane region. worker-zone=est The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=ipsum The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=ipsum Required. The job name to use for the created job. For an update job request, the job name should be the same as the existing running job. launch-options=key=est Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters. the value will be associated with the given key parameters=key=gubergren The parameters for the Flex Template. Example: {&#34;num_workers&#34;:&#34;5&#34;} the value will be associated with the given key transform-name-mappings=key=ea Use this to pass transform name mappings for streaming update jobs. Example: {&#34;oldTransformName&#34;:&#34;newTransformName&#34;,...} the value will be associated with the given key update=false Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job. .. location=lorem Required. The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. For example, us-central1 , us-west1 . project-id=eos Required. The ID of the Cloud Platform project that the job belongs to. validate-only=false If true, the request is validated but not actually executed. Defaults to false. ..dataflow-launch-template-request gcs-path=sed A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'. launch-parameters.environment additional-experiments=duo Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=sed Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs. Example: { \"name\": \"wrench\", \"mass\": \"1kg\", \"count\": \"3\" }. the value will be associated with the given key bypass-temp-dir-validation=true Whether to bypass the safety checks for the job's temporary directory. Use with caution. enable-streaming-engine=true Whether to enable Streaming Engine for the job. ip-configuration=et Configuration for VM IPs. kms-key-name=et Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/ machine-type=vero The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=70 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=sed Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=81 The initial number of Compute Engine instances for the job. service-account-email=dolore The email address of the service account to run the job as. subnetwork=et Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=voluptua. The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=amet. The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region. worker-zone=consetetur The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=diam The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=dolor Required. The job name to use for the created job. parameters=key=et The runtime parameters to pass to the job. the value will be associated with the given key transform-name-mapping=key=et Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline. the value will be associated with the given key update=false If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state. .. location=stet The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. project-id=dolor Required. The ID of the Cloud Platform project that the job belongs to. validate-only=false If true, the request is validated but not actually executed. Defaults to false. About Cursors The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up. Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Create"},{"location":"projects_locations-pipelines-create/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-create ...","title":"Scopes"},{"location":"projects_locations-pipelines-create/#required-scalar-argument","text":"<parent> (string) Required. The location name. For example: projects/PROJECT_ID/locations/LOCATION_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-create/#required-request-value","text":"The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1Pipeline: create-time: string display-name: string job-count: integer last-update-time: string name: string pipeline-sources: { string: string } schedule-info: next-job-time: string schedule: string time-zone: string scheduler-service-account-email: string state: string type: string workload: dataflow-flex-template-request: launch-parameter: container-spec-gcs-path: string environment: additional-experiments: [string] additional-user-labels: { string: string } enable-streaming-engine: boolean flexrs-goal: string ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string launch-options: { string: string } parameters: { string: string } transform-name-mappings: { string: string } update: boolean location: string project-id: string validate-only: boolean dataflow-launch-template-request: gcs-path: string launch-parameters: environment: additional-experiments: [string] additional-user-labels: { string: string } bypass-temp-dir-validation: boolean enable-streaming-engine: boolean ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string parameters: { string: string } transform-name-mapping: { string: string } update: boolean location: string project-id: string validate-only: boolean can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time. -r . create-time=et Output only. Immutable. The timestamp when the pipeline was initially created. Set by the Data Pipelines service. display-name=magna Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_). job-count=90 Output only. Number of jobs. last-update-time=ipsum Output only. Immutable. The timestamp when the pipeline was last modified. Set by the Data Pipelines service. name=voluptua. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . * PROJECT_ID can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see Identifying projects . * LOCATION_ID is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling google.cloud.location.Locations.ListLocations . Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in App Engine regions . * PIPELINE_ID is the ID of the pipeline. Must be unique for the selected project and location. pipeline-sources=key=at Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation. the value will be associated with the given key schedule-info next-job-time=sanctus Output only. When the next Scheduler job is going to run. schedule=sed Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler. time-zone=amet. Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed. .. scheduler-service-account-email=takimata Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used. state=amet. Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests. type=duo Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline. workload.dataflow-flex-template-request.launch-parameter container-spec-gcs-path=ipsum Cloud Storage path to a file with a JSON-serialized ContainerSpec as content. environment additional-experiments=gubergren Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=lorem Additional user labels to be specified for the job. Keys and values must follow the restrictions specified in the labeling restrictions . An object containing a list of key/value pairs. Example: { &#34;name&#34;: &#34;wrench&#34;, &#34;mass&#34;: &#34;1kg&#34;, &#34;count&#34;: &#34;3&#34; } . the value will be associated with the given key enable-streaming-engine=false Whether to enable Streaming Engine for the job. flexrs-goal=dolor Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs ip-configuration=ea Configuration for VM IPs. kms-key-name=ipsum Name for the Cloud KMS key for the job. Key format is: projects//locations//keyRings//cryptoKeys/ machine-type=invidunt The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=54 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=duo Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=51 The initial number of Compute Engine instances for the job. service-account-email=sed The email address of the service account to run the job as. subnetwork=ut Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=gubergren The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=rebum. The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, defaults to the control plane region. worker-zone=est The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=ipsum The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=ipsum Required. The job name to use for the created job. For an update job request, the job name should be the same as the existing running job. launch-options=key=est Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters. the value will be associated with the given key parameters=key=gubergren The parameters for the Flex Template. Example: {&#34;num_workers&#34;:&#34;5&#34;} the value will be associated with the given key transform-name-mappings=key=ea Use this to pass transform name mappings for streaming update jobs. Example: {&#34;oldTransformName&#34;:&#34;newTransformName&#34;,...} the value will be associated with the given key update=false Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job. .. location=lorem Required. The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. For example, us-central1 , us-west1 . project-id=eos Required. The ID of the Cloud Platform project that the job belongs to. validate-only=false If true, the request is validated but not actually executed. Defaults to false. ..dataflow-launch-template-request gcs-path=sed A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'. launch-parameters.environment additional-experiments=duo Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=sed Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs. Example: { \"name\": \"wrench\", \"mass\": \"1kg\", \"count\": \"3\" }. the value will be associated with the given key bypass-temp-dir-validation=true Whether to bypass the safety checks for the job's temporary directory. Use with caution. enable-streaming-engine=true Whether to enable Streaming Engine for the job. ip-configuration=et Configuration for VM IPs. kms-key-name=et Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/ machine-type=vero The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=70 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=sed Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=81 The initial number of Compute Engine instances for the job. service-account-email=dolore The email address of the service account to run the job as. subnetwork=et Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=voluptua. The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=amet. The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region. worker-zone=consetetur The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=diam The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=dolor Required. The job name to use for the created job. parameters=key=et The runtime parameters to pass to the job. the value will be associated with the given key transform-name-mapping=key=et Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline. the value will be associated with the given key update=false If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state. .. location=stet The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. project-id=dolor Required. The ID of the Cloud Platform project that the job belongs to. validate-only=false If true, the request is validated but not actually executed. Defaults to false.","title":"Required Request Value"},{"location":"projects_locations-pipelines-create/#about-cursors","text":"The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up.","title":"About Cursors"},{"location":"projects_locations-pipelines-create/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-create/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-delete/","text":"Deletes a pipeline. If a scheduler job is attached to the pipeline, it will be deleted. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-delete ... Required Scalar Argument <name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Delete"},{"location":"projects_locations-pipelines-delete/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-delete ...","title":"Scopes"},{"location":"projects_locations-pipelines-delete/#required-scalar-argument","text":"<name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-delete/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-delete/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-get/","text":"Looks up a single pipeline. Returns a \"NOT_FOUND\" error if no such pipeline exists. Returns a \"FORBIDDEN\" error if the caller doesn't have permission to access it. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-get ... Required Scalar Argument <name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Get"},{"location":"projects_locations-pipelines-get/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-get ...","title":"Scopes"},{"location":"projects_locations-pipelines-get/#required-scalar-argument","text":"<name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-get/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-get/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-jobs-list/","text":"Lists jobs for a given pipeline. Throws a \"FORBIDDEN\" error if the caller doesn't have permission to access it. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-jobs-list ... Required Scalar Argument <parent> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional Method Properties You may set the following properties to further configure the call. Please note that -p is followed by one or more key-value-pairs, and is called like this -p k1=v1 k2=v2 even though the listing below repeats the -p for completeness. -p page-size=integer The maximum number of entities to return. The service may return fewer than this value, even if there are additional pages. If unspecified, the max limit will be determined by the backend implementation. -p page-token=string A page token, received from a previous ListJobs call. Provide this to retrieve the subsequent page. When paginating, all other parameters provided to ListJobs must match the call that provided the page token. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Jobs List"},{"location":"projects_locations-pipelines-jobs-list/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-jobs-list ...","title":"Scopes"},{"location":"projects_locations-pipelines-jobs-list/#required-scalar-argument","text":"<parent> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-jobs-list/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-jobs-list/#optional-method-properties","text":"You may set the following properties to further configure the call. Please note that -p is followed by one or more key-value-pairs, and is called like this -p k1=v1 k2=v2 even though the listing below repeats the -p for completeness. -p page-size=integer The maximum number of entities to return. The service may return fewer than this value, even if there are additional pages. If unspecified, the max limit will be determined by the backend implementation. -p page-token=string A page token, received from a previous ListJobs call. Provide this to retrieve the subsequent page. When paginating, all other parameters provided to ListJobs must match the call that provided the page token.","title":"Optional Method Properties"},{"location":"projects_locations-pipelines-jobs-list/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-list/","text":"Lists pipelines. Returns a \"FORBIDDEN\" error if the caller doesn't have permission to access it. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-list ... Required Scalar Argument <parent> (string) Required. The location name. For example: projects/PROJECT_ID/locations/LOCATION_ID . Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional Method Properties You may set the following properties to further configure the call. Please note that -p is followed by one or more key-value-pairs, and is called like this -p k1=v1 k2=v2 even though the listing below repeats the -p for completeness. -p filter=string An expression for filtering the results of the request. If unspecified, all pipelines will be returned. Multiple filters can be applied and must be comma separated. Fields eligible for filtering are: + type : The type of the pipeline (streaming or batch). Allowed values are ALL , BATCH , and STREAMING . + status : The activity status of the pipeline. Allowed values are ALL , ACTIVE , ARCHIVED , and PAUSED . For example, to limit results to active batch processing pipelines: type:BATCH,status:ACTIVE -p page-size=integer The maximum number of entities to return. The service may return fewer than this value, even if there are additional pages. If unspecified, the max limit is yet to be determined by the backend implementation. -p page-token=string A page token, received from a previous ListPipelines call. Provide this to retrieve the subsequent page. When paginating, all other parameters provided to ListPipelines must match the call that provided the page token. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines List"},{"location":"projects_locations-pipelines-list/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-list ...","title":"Scopes"},{"location":"projects_locations-pipelines-list/#required-scalar-argument","text":"<parent> (string) Required. The location name. For example: projects/PROJECT_ID/locations/LOCATION_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-list/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-list/#optional-method-properties","text":"You may set the following properties to further configure the call. Please note that -p is followed by one or more key-value-pairs, and is called like this -p k1=v1 k2=v2 even though the listing below repeats the -p for completeness. -p filter=string An expression for filtering the results of the request. If unspecified, all pipelines will be returned. Multiple filters can be applied and must be comma separated. Fields eligible for filtering are: + type : The type of the pipeline (streaming or batch). Allowed values are ALL , BATCH , and STREAMING . + status : The activity status of the pipeline. Allowed values are ALL , ACTIVE , ARCHIVED , and PAUSED . For example, to limit results to active batch processing pipelines: type:BATCH,status:ACTIVE -p page-size=integer The maximum number of entities to return. The service may return fewer than this value, even if there are additional pages. If unspecified, the max limit is yet to be determined by the backend implementation. -p page-token=string A page token, received from a previous ListPipelines call. Provide this to retrieve the subsequent page. When paginating, all other parameters provided to ListPipelines must match the call that provided the page token.","title":"Optional Method Properties"},{"location":"projects_locations-pipelines-list/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-patch/","text":"Updates a pipeline. If successful, the updated Pipeline is returned. Returns NOT_FOUND if the pipeline doesn't exist. If UpdatePipeline does not return successfully, you can retry the UpdatePipeline request until you receive a successful response. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-patch ... Required Scalar Argument <name> (string) The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . * PROJECT_ID can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see Identifying projects . * LOCATION_ID is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling google.cloud.location.Locations.ListLocations . Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in App Engine regions . * PIPELINE_ID is the ID of the pipeline. Must be unique for the selected project and location. Required Request Value The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1Pipeline: create-time: string display-name: string job-count: integer last-update-time: string name: string pipeline-sources: { string: string } schedule-info: next-job-time: string schedule: string time-zone: string scheduler-service-account-email: string state: string type: string workload: dataflow-flex-template-request: launch-parameter: container-spec-gcs-path: string environment: additional-experiments: [string] additional-user-labels: { string: string } enable-streaming-engine: boolean flexrs-goal: string ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string launch-options: { string: string } parameters: { string: string } transform-name-mappings: { string: string } update: boolean location: string project-id: string validate-only: boolean dataflow-launch-template-request: gcs-path: string launch-parameters: environment: additional-experiments: [string] additional-user-labels: { string: string } bypass-temp-dir-validation: boolean enable-streaming-engine: boolean ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string parameters: { string: string } transform-name-mapping: { string: string } update: boolean location: string project-id: string validate-only: boolean can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time. -r . create-time=vero Output only. Immutable. The timestamp when the pipeline was initially created. Set by the Data Pipelines service. display-name=invidunt Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_). job-count=36 Output only. Number of jobs. last-update-time=vero Output only. Immutable. The timestamp when the pipeline was last modified. Set by the Data Pipelines service. name=elitr The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . * PROJECT_ID can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see Identifying projects . * LOCATION_ID is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling google.cloud.location.Locations.ListLocations . Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in App Engine regions . * PIPELINE_ID is the ID of the pipeline. Must be unique for the selected project and location. pipeline-sources=key=lorem Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation. the value will be associated with the given key schedule-info next-job-time=diam Output only. When the next Scheduler job is going to run. schedule=no Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler. time-zone=ipsum Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed. .. scheduler-service-account-email=accusam Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used. state=takimata Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests. type=consetetur Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline. workload.dataflow-flex-template-request.launch-parameter container-spec-gcs-path=voluptua. Cloud Storage path to a file with a JSON-serialized ContainerSpec as content. environment additional-experiments=et Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=erat Additional user labels to be specified for the job. Keys and values must follow the restrictions specified in the labeling restrictions . An object containing a list of key/value pairs. Example: { &#34;name&#34;: &#34;wrench&#34;, &#34;mass&#34;: &#34;1kg&#34;, &#34;count&#34;: &#34;3&#34; } . the value will be associated with the given key enable-streaming-engine=false Whether to enable Streaming Engine for the job. flexrs-goal=amet. Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs ip-configuration=sed Configuration for VM IPs. kms-key-name=takimata Name for the Cloud KMS key for the job. Key format is: projects//locations//keyRings//cryptoKeys/ machine-type=dolores The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=39 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=et Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=78 The initial number of Compute Engine instances for the job. service-account-email=voluptua. The email address of the service account to run the job as. subnetwork=dolore Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=dolore The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=dolore The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, defaults to the control plane region. worker-zone=voluptua. The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=amet. The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=ea Required. The job name to use for the created job. For an update job request, the job name should be the same as the existing running job. launch-options=key=sadipscing Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters. the value will be associated with the given key parameters=key=lorem The parameters for the Flex Template. Example: {&#34;num_workers&#34;:&#34;5&#34;} the value will be associated with the given key transform-name-mappings=key=invidunt Use this to pass transform name mappings for streaming update jobs. Example: {&#34;oldTransformName&#34;:&#34;newTransformName&#34;,...} the value will be associated with the given key update=true Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job. .. location=sit Required. The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. For example, us-central1 , us-west1 . project-id=et Required. The ID of the Cloud Platform project that the job belongs to. validate-only=true If true, the request is validated but not actually executed. Defaults to false. ..dataflow-launch-template-request gcs-path=aliquyam A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'. launch-parameters.environment additional-experiments=ipsum Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=et Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs. Example: { \"name\": \"wrench\", \"mass\": \"1kg\", \"count\": \"3\" }. the value will be associated with the given key bypass-temp-dir-validation=true Whether to bypass the safety checks for the job's temporary directory. Use with caution. enable-streaming-engine=true Whether to enable Streaming Engine for the job. ip-configuration=et Configuration for VM IPs. kms-key-name=sed Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/ machine-type=no The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=16 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=elitr Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=21 The initial number of Compute Engine instances for the job. service-account-email=no The email address of the service account to run the job as. subnetwork=nonumy Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=at The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=sadipscing The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region. worker-zone=aliquyam The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=dolores The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=sadipscing Required. The job name to use for the created job. parameters=key=erat The runtime parameters to pass to the job. the value will be associated with the given key transform-name-mapping=key=aliquyam Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline. the value will be associated with the given key update=true If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state. .. location=est The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. project-id=et Required. The ID of the Cloud Platform project that the job belongs to. validate-only=false If true, the request is validated but not actually executed. Defaults to false. About Cursors The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up. Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional Method Properties You may set the following properties to further configure the call. Please note that -p is followed by one or more key-value-pairs, and is called like this -p k1=v1 k2=v2 even though the listing below repeats the -p for completeness. -p update-mask=string The list of fields to be updated. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Patch"},{"location":"projects_locations-pipelines-patch/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-patch ...","title":"Scopes"},{"location":"projects_locations-pipelines-patch/#required-scalar-argument","text":"<name> (string) The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . * PROJECT_ID can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see Identifying projects . * LOCATION_ID is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling google.cloud.location.Locations.ListLocations . Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in App Engine regions . * PIPELINE_ID is the ID of the pipeline. Must be unique for the selected project and location.","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-patch/#required-request-value","text":"The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1Pipeline: create-time: string display-name: string job-count: integer last-update-time: string name: string pipeline-sources: { string: string } schedule-info: next-job-time: string schedule: string time-zone: string scheduler-service-account-email: string state: string type: string workload: dataflow-flex-template-request: launch-parameter: container-spec-gcs-path: string environment: additional-experiments: [string] additional-user-labels: { string: string } enable-streaming-engine: boolean flexrs-goal: string ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string launch-options: { string: string } parameters: { string: string } transform-name-mappings: { string: string } update: boolean location: string project-id: string validate-only: boolean dataflow-launch-template-request: gcs-path: string launch-parameters: environment: additional-experiments: [string] additional-user-labels: { string: string } bypass-temp-dir-validation: boolean enable-streaming-engine: boolean ip-configuration: string kms-key-name: string machine-type: string max-workers: integer network: string num-workers: integer service-account-email: string subnetwork: string temp-location: string worker-region: string worker-zone: string zone: string job-name: string parameters: { string: string } transform-name-mapping: { string: string } update: boolean location: string project-id: string validate-only: boolean can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time. -r . create-time=vero Output only. Immutable. The timestamp when the pipeline was initially created. Set by the Data Pipelines service. display-name=invidunt Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_). job-count=36 Output only. Number of jobs. last-update-time=vero Output only. Immutable. The timestamp when the pipeline was last modified. Set by the Data Pipelines service. name=elitr The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . * PROJECT_ID can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see Identifying projects . * LOCATION_ID is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling google.cloud.location.Locations.ListLocations . Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in App Engine regions . * PIPELINE_ID is the ID of the pipeline. Must be unique for the selected project and location. pipeline-sources=key=lorem Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation. the value will be associated with the given key schedule-info next-job-time=diam Output only. When the next Scheduler job is going to run. schedule=no Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler. time-zone=ipsum Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed. .. scheduler-service-account-email=accusam Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used. state=takimata Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests. type=consetetur Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline. workload.dataflow-flex-template-request.launch-parameter container-spec-gcs-path=voluptua. Cloud Storage path to a file with a JSON-serialized ContainerSpec as content. environment additional-experiments=et Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=erat Additional user labels to be specified for the job. Keys and values must follow the restrictions specified in the labeling restrictions . An object containing a list of key/value pairs. Example: { &#34;name&#34;: &#34;wrench&#34;, &#34;mass&#34;: &#34;1kg&#34;, &#34;count&#34;: &#34;3&#34; } . the value will be associated with the given key enable-streaming-engine=false Whether to enable Streaming Engine for the job. flexrs-goal=amet. Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs ip-configuration=sed Configuration for VM IPs. kms-key-name=takimata Name for the Cloud KMS key for the job. Key format is: projects//locations//keyRings//cryptoKeys/ machine-type=dolores The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=39 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=et Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=78 The initial number of Compute Engine instances for the job. service-account-email=voluptua. The email address of the service account to run the job as. subnetwork=dolore Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=dolore The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=dolore The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, defaults to the control plane region. worker-zone=voluptua. The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=amet. The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=ea Required. The job name to use for the created job. For an update job request, the job name should be the same as the existing running job. launch-options=key=sadipscing Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters. the value will be associated with the given key parameters=key=lorem The parameters for the Flex Template. Example: {&#34;num_workers&#34;:&#34;5&#34;} the value will be associated with the given key transform-name-mappings=key=invidunt Use this to pass transform name mappings for streaming update jobs. Example: {&#34;oldTransformName&#34;:&#34;newTransformName&#34;,...} the value will be associated with the given key update=true Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job. .. location=sit Required. The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. For example, us-central1 , us-west1 . project-id=et Required. The ID of the Cloud Platform project that the job belongs to. validate-only=true If true, the request is validated but not actually executed. Defaults to false. ..dataflow-launch-template-request gcs-path=aliquyam A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'. launch-parameters.environment additional-experiments=ipsum Additional experiment flags for the job. Each invocation of this argument appends the given value to the array. additional-user-labels=key=et Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs. Example: { \"name\": \"wrench\", \"mass\": \"1kg\", \"count\": \"3\" }. the value will be associated with the given key bypass-temp-dir-validation=true Whether to bypass the safety checks for the job's temporary directory. Use with caution. enable-streaming-engine=true Whether to enable Streaming Engine for the job. ip-configuration=et Configuration for VM IPs. kms-key-name=sed Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/ machine-type=no The machine type to use for the job. Defaults to the value from the template if not specified. max-workers=16 The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000. network=elitr Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\". num-workers=21 The initial number of Compute Engine instances for the job. service-account-email=no The email address of the service account to run the job as. subnetwork=nonumy Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK\" or \"regions/REGION/subnetworks/SUBNETWORK\". If the subnetwork is located in a Shared VPC network, you must use the complete URL. temp-location=at The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs:// . worker-region=sadipscing The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region. worker-zone=aliquyam The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity. If both worker_zone and zone are set, worker_zone takes precedence. zone=dolores The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, worker_zone will take precedence. .. job-name=sadipscing Required. The job name to use for the created job. parameters=key=erat The runtime parameters to pass to the job. the value will be associated with the given key transform-name-mapping=key=aliquyam Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline. the value will be associated with the given key update=true If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state. .. location=est The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. project-id=et Required. The ID of the Cloud Platform project that the job belongs to. validate-only=false If true, the request is validated but not actually executed. Defaults to false.","title":"Required Request Value"},{"location":"projects_locations-pipelines-patch/#about-cursors","text":"The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up.","title":"About Cursors"},{"location":"projects_locations-pipelines-patch/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-patch/#optional-method-properties","text":"You may set the following properties to further configure the call. Please note that -p is followed by one or more key-value-pairs, and is called like this -p k1=v1 k2=v2 even though the listing below repeats the -p for completeness. -p update-mask=string The list of fields to be updated.","title":"Optional Method Properties"},{"location":"projects_locations-pipelines-patch/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-run/","text":"Creates a job for the specified pipeline directly. You can use this method when the internal scheduler is not configured and you want to trigger the job directly or through an external system. Returns a \"NOT_FOUND\" error if the pipeline doesn't exist. Returns a \"FORBIDDEN\" error if the user doesn't have permission to access the pipeline or run jobs for the pipeline. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-run ... Required Scalar Argument <name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . Required Request Value The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1RunPipelineRequest: can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time. About Cursors The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up. Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Run"},{"location":"projects_locations-pipelines-run/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-run ...","title":"Scopes"},{"location":"projects_locations-pipelines-run/#required-scalar-argument","text":"<name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-run/#required-request-value","text":"The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1RunPipelineRequest: can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time.","title":"Required Request Value"},{"location":"projects_locations-pipelines-run/#about-cursors","text":"The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up.","title":"About Cursors"},{"location":"projects_locations-pipelines-run/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-run/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"},{"location":"projects_locations-pipelines-stop/","text":"Freezes pipeline execution permanently. If there's a corresponding scheduler entry, it's deleted, and the pipeline state is changed to \"ARCHIVED\". However, pipeline metadata is retained. Scopes You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-stop ... Required Scalar Argument <name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID . Required Request Value The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1StopPipelineRequest: can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time. About Cursors The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up. Optional Output Flags The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output. Optional General Properties The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Locations Pipelines Stop"},{"location":"projects_locations-pipelines-stop/#scopes","text":"You will need authorization for the https://www.googleapis.com/auth/cloud-platform scope to make a valid call. If unset, the scope for this method defaults to https://www.googleapis.com/auth/cloud-platform . You can set the scope for this method like this: datapipelines1 --scope <scope> projects locations-pipelines-stop ...","title":"Scopes"},{"location":"projects_locations-pipelines-stop/#required-scalar-argument","text":"<name> (string) Required. The pipeline name. For example: projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID .","title":"Required Scalar Argument"},{"location":"projects_locations-pipelines-stop/#required-request-value","text":"The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure. In the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely. For example, a structure like this: GoogleCloudDatapipelinesV1StopPipelineRequest: can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time.","title":"Required Request Value"},{"location":"projects_locations-pipelines-stop/#about-cursors","text":"The cursor position is key to comfortably set complex nested structures. The following rules apply: The cursor position is always set relative to the current one, unless the field name starts with the . character. Fields can be nested such as in -r f.s.o . The cursor position is set relative to the top-level structure if it starts with . , e.g. -r .s.s You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify -r struct.sub_struct=bar . You can move the cursor one level up by using .. . Each additional . moves it up one additional level. E.g. ... would go three levels up.","title":"About Cursors"},{"location":"projects_locations-pipelines-stop/#optional-output-flags","text":"The method's return value a JSON encoded structure, which will be written to standard output by default. -o out out specifies the destination to which to write the server's result to. It will be a JSON-encoded structure. The destination may be - to indicate standard output, or a filepath that is to contain the received bytes. If unset, it defaults to standard output.","title":"Optional Output Flags"},{"location":"projects_locations-pipelines-stop/#optional-general-properties","text":"The following properties can configure any call, and are not specific to this method. -p $-xgafv=string V1 error format. -p access-token=string OAuth access token. -p alt=string Data format for response. -p callback=string JSONP -p fields=string Selector specifying which fields to include in a partial response. -p key=string API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token. -p oauth-token=string OAuth 2.0 token for the current user. -p pretty-print=boolean Returns response with indentations and line breaks. -p quota-user=string Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters. -p upload-type=string Legacy upload protocol for media (e.g. \"media\", \"multipart\"). -p upload-protocol=string Upload protocol for media (e.g. \"raw\", \"multipart\").","title":"Optional General Properties"}]}